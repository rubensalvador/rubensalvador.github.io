---
---

@inproceedings{lee_AeroConf_2024,
  abbr={AeroConf},
  bibtex_show={true},
  title = {On-Board {{Payload Data Processing Combined}} with the {{Roofline Model}} for {{Hardware}}/{{Software Design}}},
  booktitle = {{{AeroConf}} 2024 - {{IEEE Aerospace Conference}}},
  author = {Lee, Seungah and Casseau, Emmanuel and Kritikakou, Angeliki and Sentieys, Olivier and Salvador, Ruben and Galizzi, Julien},
  year = {2024},
  month = mar,
  pages = {1},
  langid = {english},
  html = {https://inria.hal.science/hal-04423185/},
  abstract = { High-performance on-board payload data processing has become more interesting with the development of radiationhardened multiprocessor System-on-chip (MPSoC). As recent space-qualified MPSoCs include Arm Central Processing Units (CPUs) and Field Programmable Gate Arrays (FPGAs), an efficient design method is required to deal with complex heterogeneous embedded systems. Both data bit-width (data accuracy) and processing performance are important in astronomy, thus the design methodology should concern application-specific Multi-Objective Optimization Problems (MOOPs). This paper proposes to combine the roofline performance model with Design Space Exploration (DSE) of hardware/software designs as a methodology. We use High-Level Synthesis (HLS) for FPGA design to configure different hardware architectures based on C/C++ and pragmas. We develop a benchmark for payload data processing on Arm CPUs and embedded FPGA on a heterogeneous MPSoC by adapting open-source libraries for one of the most commonly used algorithms to provide validated libraries to payload teams. The benchmark takes as constraints the SVOM ECLAIRs payload requirements, and as input data the CCSDS test images, executes applications, and verifies output data. We chose an AMD-Xilinx Zynq UltraScale+ evaluation board and the two-Dimensional Fast Fourier Transform (2-D FFT) as a DSE use case. We designed the benchmark on an Arm Cortex-A53 in bare-metal and an embedded FPGA based on Vitis HLS. The results show a customized roofline model with the hardware/software design. The implemented design has 1.6-55 times faster performance compared to the payload execution time requirement. Based on the proposed roofline model and the DSE results, future payload teams can study the trade-off between execution time and area efficiency to select the most suitable implementation.}
}

@INPROCEEDINGS{lee_EDHPC_2023,
  abbr={EDHPC},
  bibtex_show={true},
  author={Lee, Seungah and Sentieys, Olivier and Salvador, Ruben and Galizzi, Julien and Kritikakou, Angeliki and Casseau, Emmanuel},
  booktitle={2023 European Data Handling & Data Processing Conference (EDHPC)},
  title={High-Level Synthesis-Based On-board Payload Data Processing considering the Roofline Model},
  year={2023},
  volume={},
  number={},
  pages={1-7},
  doi={10.23919/EDHPC59100.2023.10396136},
  html = {https://inria.hal.science/hal-04294305/},
  abstract = {On-board payload data processing can be performed by developing space-qualified heterogeneous Multiprocessor System-on-Chips (MPSoCs). We present key compute-intensive payload algorithms, based on a survey with space science researchers, including the two-dimensional Fast Fourier Transform (2-D FFT). Also, we propose to perform design space exploration by combining the roofline performance model with High-Level Synthesis (HLS) for hardware accelerator architecture design. The roofline model visualizes the limits of a given architecture regarding Input/Output (I/O) bandwidth and computational performance, along with the achieved performance for different implementations. HLS is an interesting option in developing FPGA-based on-board processing applications for payload teams that need to adjust architecture specifications through design reviews and have limited expertise in Hardware Description Languages (HDLs). In this paper, we focus on an FPGA-based MPSoC thanks to recently released radiation-hardened heterogeneous embedded platforms.}
}


@inproceedings{guillaume_CARDIS_2023,
  selected={true},
  abbr={CARDIS},
  bibtex_show={true},
  title = {Attacking at~{{Non-harmonic Frequencies}} in~{{Screaming-Channel Attacks}}},
  booktitle = {Smart {{Card Research}} and {{Advanced Applications}}},
  author = {Guillaume, Jeremy and Pelcat, Maxime and Nafkha, Amor and Salvador, Rub{\'e}n},
  editor = {Bhasin, Shivam and Roche, Thomas},
  year = {2023},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {87--106},
  publisher = {{Springer Nature Switzerland}},
  address = {{Cham}},
  doi = {10.1007/978-3-031-54409-5_5},
  copyright = {All rights reserved},
  isbn = {978-3-031-54409-5},
  langid = {english},
  arxiv = {2311.15832},
  abstract = {Screaming-channel attacks enable Electromagnetic (EM) Side-Channel Attacks (SCAs) at larger distances due to higher EM leakage energies than traditional SCAs, relaxing the requirement of close access to the victim. This attack can be mounted on devices integrating Radio Frequency (RF) modules on the same die as digital circuits, where the RF can unintentionally capture, modulate, amplify, and transmit the leakage along with legitimate signals. Leakage results from digital switching activity, so previous works hypothesized that this leakage would appear at multiples of the digital clock frequency, i.e., harmonics.
  This work demonstrates that compromising signals appear not only at the harmonics and that leakage at non-harmonics can be exploited for successful attacks. Indeed, the transformations undergone by the leaked signal are complex due to propagation effects through the substrate and power and ground planes, so the leakage also appears at other frequencies. We first propose two methodologies to locate frequencies that contain leakage and demonstrate that it appears at non-harmonic frequencies. Then, our experimental results show that screaming-channel attacks at non-harmonic frequencies can be as successful as at harmonics when retrieving a 16-byte AES key. As the RF spectrum is polluted by interfering signals, we run experiments and show successful attacks in a more realistic, noisy environment where harmonic frequencies are contaminated by multi-path fading and interference. These attacks at non-harmonic frequencies increase the attack surface by providing attackers with more potential frequencies where attacks can succeed.}
}

@inproceedings{khosroshahi2023assessment,
  abbr={IWAIT},
  bibtex_show={true},
  title={Assessment of multi-plenoptic 2.0 camera depth maps for DIBR},
  author={Khosroshahi, Hamed Razavi and Sancho, Jaime and Rosa, Gonzalo and Salvador, Rubén and Juarez, Eduardo and Lafruit, Gauthier and Teratani, Mehrdad},
  booktitle={International Workshop on Advanced Imaging Technology (IWAIT) 2023},
  volume={12592},
  pages={279--284},
  year={2023},
  organization={SPIE},
  abstract = {We aim to evaluate the depth map quality by the plenoptic 2.0 cameras for DIBR applications, by simulating a multi-plenoptic camera array, and synthesizing virtual images based on the obtained color and depth maps. The real-time generated depth maps by plenoptic cameras are compared with the offline and high-quality depth maps estimated by the MPEG-I Depth Estimation Reference Software (DERS). Results show that synthesized virtual image quality obtained by RGBD plenoptic cameras has lower quality than using the depth estimated by DERS, due to a loss of inter-view depth consistency.}
}

@inproceedings{guillaume_VT_SiPS_2022, 
   selected={true},
   abbr={SiPS},
   bibtex_show={true},
   author={Guillaume, Jeremy and Pelcat, Maxime and Nafkha, Amor and Salvador, Ruben},  
   booktitle={2022 IEEE Workshop on Signal Processing Systems (SiPS)},
   title={Virtual Triggering: a Technique to Segment Cryptographic Processes in Side-Channel Traces},
   year={2022},
   volume={},
   number={},
   pages={1-6},
   doi={10.1109/SiPS55645.2022.9919238},
   arxiv={2210.12059},
   abstract={{Side-Channel Attacks (SCAs) exploit data correlation in signals leaked from devices to jeopardize confidentiality. Locating and synchronizing segments of interest in traces from Cryptographic Processes (CPs) is a key step of the attack. The most common method consists in generating a trigger signal to indicate to the attacker the start of a CP. This paper proposes a method called Virtual Triggering (VT) that removes the need for the trigger signal and automates trace segmentation. When the time between repetitions is not constant, further trace alignment techniques are required. Building on VT, we propose a simple method to learn representative segment templates from a profiling device similar to the victim, and to automatically locate and pull out these segments from other victim devices using simple pattern recognition. We evaluate VT on screaming channel attacks [1], which initially used a Frequency Component (FC) known to appear at a single time in leaked signals, as a trigger to segment traces. We demonstrate that VT not only performs equivalently to FC on a standard attack scenario, but we also show how using VT with the automatic pullout technique improves the attack efficiency and enables more realistic attack scenarios. Thanks to VT, screaming channel attacks can now: (1) succeed with only half of the segments collected compared to the FC trigger from the original attack; and (2) absorb time variations between CPs.}}
}

@inproceedings{villa_DASIP_2022,
   abbr={DASIP},
   bibtex_show={true},
   title={Data-Type Assessment for Real-Time Hyperspectral Classification in Medical Imaging},
   author={Villa, Manuel and Sancho, Jaime and Vazquez, Guillermo and Rosa, Gonzalo and Urbanos, Gemma and Martin-Perez, Alberto and Sutradhar, Pallab and Salvador, Rubén and Chavarrías, Miguel and Lagares, Alfonso and Ju{\'a}rez, Eduardo and Sanz, C{\'e}sar},
   booktitle={Design and Architecture for Signal and Image Processing},
   doi = {10.1007/978-3-031-12748-9_10},
   isbn = {978-3-031-12748-9},
   year={2022},
   pages={123--135},
   abstract={{Real-time constraints in image processing applications often force their optimization using hardware accelerators. This is the case for intraoperative medical images used during surgical procedures. In this context, the challenge consists in processing large volumes of data while employing high complexity algorithms in a limited period of time. Newly developed algorithms must meet both quality-accurate and hardware-efficient characteristics. In this work, we have evaluated the impact of using different data types in a processing chain to classify tissues from hyperspectral video in surgical environments. The software was run on two different embedded CPU+GPU platforms. The results show an improvement in performance by up to 9 times without increasing power consumption by reducing the bit depth from 64 to 16. The impact these reduction have on quality has been measured analytically, by calculating the RMSE, and subjectively, by surveying neurosurgeons. In both cases the results show a minimal impact on the overall quality.}}
}

@article{sancho_Sensors_2021,
   abbr={Sensors},
   bibtex_show={true},
   title={GoRG: Towards a GPU-Accelerated Multiview Hyperspectral Depth Estimation Tool for Medical Applications},
   author={Sancho, Jaime and Sutradhar, Pallab and Rosa, Gonzalo and Chavarrías, Miguel and Perez-Nu{\~n}ez, Angel and Salvador, Rubén and Lagares, Alfonso and Ju{\'a}rez, Eduardo and Sanz, C{\'e}sar},
   journal={Sensors},
   volume={21},
   number={12},
   pages={4091},
   year={2021},
   publisher={Multidisciplinary Digital Publishing Institute},
   doi = {10.3390/s21124091},
   abstract = {{ HyperSpectral (HS) images have been successfully used for brain tumor boundary detection during resection operations. Nowadays, these classification maps coexist with other technologies such as MRI or IOUS that improve a neurosurgeon’s action, with their incorporation being a neurosurgeon’s task. The project in which this work is framed generates an unified and more accurate 3D immersive model using HS, MRI, and IOUS information. To do so, the HS images need to include 3D information and it needs to be generated in real-time operating room conditions, around a few seconds. This work presents Graph cuts Reference depth estimation in GPU (GoRG), a GPU-accelerated multiview depth estimation tool for HS images also able to process YUV images in less than 5.5 s on average. Compared to a high-quality SoA algorithm, MPEG DERS, GoRG YUV obtain quality losses of −0.93 dB, −0.6 dB, and −1.96% for WS-PSNR, IV-PSNR, and VMAF, respectively, using a video synthesis processing chain. For HS test images, GoRG obtains an average RMSE of 7.5 cm, with most of its errors in the background, needing around 850 ms to process one frame and view. These results demonstrate the feasibility of using GoRG during a tumor resection operation.}}
}

@article{mendez_AppliedSciences_2021,
   selected={true},
   abbr={Appl. Sci.},
   bibtex_show={true},
   title={Physical Side-Channel Attacks on Embedded Neural Networks: A Survey},
   author={M{\'e}ndez Real, Maria and Salvador, Rubén},
   journal={Applied Sciences},
   volume={11},
   number={15},
   pages={6790},
   year={2021},
   publisher={Multidisciplinary Digital Publishing Institute},
   doi = {10.3390/app11156790},
   arxiv = {2110.11290},
   html = {https://inria.hal.science/hal-03324220/},
   abstract = {{ During the last decade, Deep Neural Networks (DNN) have progressively been integrated on all types of platforms, from data centers to embedded systems including low-power processors and, recently, FPGAs. Neural Networks (NN) are expected to become ubiquitous in IoT systems by transforming all sorts of real-world applications, including applications in the safety-critical and security-sensitive domains. However, the underlying hardware security vulnerabilities of embedded NN implementations remain unaddressed. In particular, embedded DNN implementations are vulnerable to Side-Channel Analysis (SCA) attacks, which are especially important in the IoT and edge computing contexts where an attacker can usually gain physical access to the targeted device. A research field has therefore emerged and is rapidly growing in terms of the use of SCA including timing, electromagnetic attacks and power attacks to target NN embedded implementations. Since 2018, research papers have shown that SCA enables an attacker to recover inference models architectures and parameters, to expose industrial IP and endangers data confidentiality and privacy. Without a complete review of this emerging field in the literature so far, this paper surveys state-of-the-art physical SCA attacks relative to the implementation of embedded DNNs on micro-controllers and FPGAs in order to provide a thorough analysis on the current landscape. It provides a taxonomy and a detailed classification of current attacks. It first discusses mitigation techniques and then provides insights for future research leads.}}
}

@inproceedings{sancho_DCIS_2021,
   abbr={DCIS},
   bibtex_show={true},
   author={Sancho, Jaime and Villa, Manuel and Urbanos, Gemma and Villanueva, Marta and Sutradhar, Pallab and Rosa, Gonzalo and Martin, Alberto and Vazquez, Guillermo and Chavarrias, Miguel and Salvador, Ruben and Lagares, Alfonso and Juarez, Eduardo and Sanz, Cesar},
   booktitle={2021 XXXVI Conference on Design of Circuits and Integrated Systems (DCIS)}, 
   title={An Embedded GPU Accelerated Hyperspectral Video Classification System in Real-Time}, 
   year={2021},
   volume={},
   number={},
   pages={1-6},
   doi={10.1109/DCIS53048.2021.9666171},
   abstract = {{Uyperspectral imaging (HSI) has been adopted during the last years in different applications where material classification plays an important role. This favoured the improvement and development of new HS sensors, leading to HS snapshot cameras, where the sensor is able to acquire images in a realtime video fashion, reducing the spatial and spectral resolution for these cameras. However, although these cameras are able to acquire RAW HS images at high frames per second (FPS) rates, it is necessary to pre-process them to obtain an actual HS image and then obtain a classification map. This work addresses the development of an embedded CPU+GPU HS video classification system able to acquire and classify HS images in real-time video, i.e., at more than 25 FPS. Also, it includes a comparison where three different CPU+GPU embedded platforms are tested: NVIDIA Jetson Nano, NVIDIA Jetson TX1 and NVIDIA Jetson TX2. The results obtained show the feasibility of a HS video classification system using these embedded platforms.}}
}

@inproceedings{sancho_DCIS_2020,
   abbr={DCIS},
   bibtex_show={true},
   author={Sancho, Jaime and Urbanos, Gemma and Ruiz, Luisa and Villanueva, Marta and Rosa, Gonzalo and Diaz, Alberto and Villa, Manuel and Chavarrías, Miguel and Lagares, Alfonso and Salvador, Rubén and Juárez, Eduardo and Sanz, César},
   booktitle={XXXV Conference on Design of Circuits and Integrated Systems (DCIS)}, 
   title={Towards GPU Accelerated HyperSpectral Depth Estimation in Medical Applications}, 
   year={2020},
   volume={},
   number={},
   pages={1-6},
   doi={10.1109/DCIS51330.2020.9268649},
   abstract = {{HyperSpectral (HS) images are commonly used for classification tasks in different domains, such as medicine. In this field, a recent use is the differentiation between healthy tissues and different types of cancerous tissues. To this end, different machine learning techniques have been proposed to generate classification maps that indicate the type of tissue corresponding to each pixel in the image. These 2D representations can be used stand-alone, but they can not be properly registered with other valuable data sources like Magnetic Resonance Imaging (MRI), which can improve the accuracy of the system. For this reason, this paper builds the foundations of a multi-modal classification system that will incorporate 3D information into HS images. Specifically, we address the acceleration of one of the hotspots in depth estimation tools/algorithms. MPEG-I Depth Estimation Reference Software (DERS) provides high-quality depth maps relying on a global energy optimizer algorithm: Graph Cuts. However, this algorithm needs huge processing times, preventing its use during surgical operations. This work introduces GoRG (Graph cuts Reference depth estimation in GPU), a GPU accelerated DERS able to produce depth maps from RGB and HS images. In this paper, due to the lack of HS multi-view datasets at the moment, results are reported on RGB images to validate the acceleration strategy. GoRG shows a ×25 average speed-up compared to baseline DERS 8.0, reducing total computation time from around one hour for 8 frames to only a few minutes. A consequence of our parallelization is an average decrease of 1.6 dB in Weighted-to-Spherically-Uniform Peak-Signal-to-Noise-Ratio (WS-PSNR), with some remarkable disparities approaching 4 dB. However, using Structural Similarity Index (SSIM) as metric results come closer to baseline DERS. Effectively, an average decrease of only 1.20% is achieved showing that the obtained speed-up gains compensate the subjective quality losses.}}
}

@article{madronal_Access_2019,
   abbr={IEEE Access},
   bibtex_show={true},
   author={Madroñal, Daniel and Arrestier, Florian and Sancho, Jaime and Morvan, Antoine and Lazcano, Raquel and Desnos, Karol and Salvador, Rubén and Menard, Daniel and Juarez, Eduardo and Sanz, Cesar},
   journal={IEEE Access}, 
   title={PAPIFY: Automatic Instrumentation and Monitoring of Dynamic Dataflow Applications Based on PAPI}, 
   year={2019},
   volume={7},
   number={},
   pages={111801-111812},
   publisher   = {IEEE},
   doi={10.1109/ACCESS.2019.2934223},
   abstract = {{The widening of the complexity-productivity gap in application development witnessed in the last years is becoming an important issue for the developers. New design methods try to automate most designers tasks to bridge this gap. In addition, new Model of Computations (MoCs), as those dataflow-based, ease the expression of parallelism within applications, leading to higher designer productivity. Rapid prototyping design tools offer fast estimations of the soundness of design choices. A key step when prototyping an application is to have representative performance indicators to estimate the validity of those design choices. Such indicators can be obtained using hardware information, while new libraries, e.g.,Performance Application Programming Interface (PAPI), ease the access to such hardware information. In this work, PAPIFY toolbox is presented as a tool to perform automatic PAPI-based instrumentation of dynamic dataflow applications. It combines PAPIFY with a dataflow Y-chart based design framework, which is called PREESM, and its companion run-time reconfiguration manager, which is called Synchronous Parameterized and Interfaced Dataflow Embedded Runtime (SPiDER). PAPIFY toolbox accounts for an automatic code generator for static and dynamic applications, a dedicated library to manage the monitoring at run-time and two User Interfaces (UIs) to ease both the configuration and the analysis of the captured run-time information. Additionally, its main advantages are 1) its capability of adapting the monitoring according to the system status and 2) adaptation of the monitoring accordingly to application workload redistribution in run-time. A thorough overhead characterization using Sobel-morpho and Stereo-matching dataflow applications shows that PAPIFY run-time monitoring overhead is up to 10%.}}
}

@article{lazcano_Access_2019,
   abbr={IEEE Access},
   bibtex_show={true},
   author={Lazcano, Raquel and Madroñal, Daniel and Florimbi, Giordana and Sancho, Jaime and Sanchez, Sergio and Leon, Raquel and Fabelo, Himar and Ortega, Samuel and Torti, Emanuele and Salvador, Rubén and Marrero-Martin, Margarita and Leporati, Francesco and Juarez, Eduardo and Callico, Gustavo M. and Sanz, Cesar},
   journal={IEEE Access}, 
   title={Parallel Implementations Assessment of a Spatial-Spectral Classifier for Hyperspectral Clinical Applications},
   year={2019},
   volume={7},
   number={},
   pages={152316-152333},
   publisher   = {IEEE},
   doi={10.1109/ACCESS.2019.2938708},
   abstract = {{Hyperspectral (HS) imaging presents itself as a non-contact, non-ionizing and non-invasive technique, proven to be suitable for medical diagnosis. However, the volume of information contained in these images makes difficult providing the surgeon with information about the boundaries in real-time. To that end, High-Performance-Computing (HPC) platforms become necessary. This paper presents a comparison between the performances provided by five different HPC platforms while processing a spatial-spectral approach to classify HS images, assessing their main benefits and drawbacks. To provide a complete study, two different medical applications, with two different requirements, have been analyzed. The first application consists of HS images taken from neurosurgical operations; the second one presents HS images taken from dermatological interventions. While the main constraint for neurosurgical applications is the processing time, in other environments, as the dermatological one, other requirements can be considered. In that sense, energy efficiency is becoming a major challenge, since this kind of applications are usually developed as hand-held devices, thus depending on the battery capacity. These requirements have been considered to choose the target platforms: on the one hand, three of the most powerful Graphic Processing Units (GPUs) available in the market; and, on the other hand, a low-power GPU and a manycore architecture, both specifically thought for being used in battery-dependent environments.}}
}

@article{fabelo_access_2019,
   abbr={IEEE Access},
   bibtex_show={true},
   author      = {H. {Fabelo} and S. {Ortega} and A. {Szolna} and D. {Bulters} and J. F. {Piñeiro} and S. {Kabwama} and A. {J-O’Shanahan} and H. {Bulstrode} and S. {Bisshopp} and B. R. {Kiran} and D. {Ravi} and R. {Lazcano} and D. {Madroñal} and C. {Sosa} and C. {Espino} and M. {Marquez} and M. d. L. L. {Plaza} and R. {Camacho} and D. {Carrera} and M. {Hernández} and G. M. {Callicó} and J. {Morera} and B. {Stanciulescu} and G. {Yang} and Salvador, Rubén and E. {Juárez} and C. {Sanz} and R. {Sarmiento}},
   journal= {IEEE Access},
   title       = {In-Vivo Hyperspectral Human Brain Image Database for Brain Cancer Detection},
   year        = {2019},
   volume      = {7},
   number      = {},
   publisher   = {IEEE},
   pages       = {39098--39116},
   doi         = {10.1109/ACCESS.2019.2904788},
   issn        = {2169-3536},
   month       = {},
   abstract    = {{The use of hyperspectral imaging for medical applications is becoming more common in recent years. One of the main obstacles that researchers find when developing hyperspectral algorithms for medical applications is the lack of specific, publicly available, and hyperspectral medical data. The work described in this paper was developed within the framework of the European project HELICoiD ( HypErspectraL Imaging Cancer Detection ), which had as a main goal the application of hyperspectral imaging to the delineation of brain tumors in real-time during neurosurgical operations. In this paper, the methodology followed to generate the first hyperspectral database of in-vivo human brain tissues is presented. Data was acquired employing a customized hyperspectral acquisition system capable of capturing information in the Visual and Near InfraRed (VNIR) range from 400 to 1000 nm. Repeatability was assessed for the cases where two images of the same scene were captured consecutively. The analysis reveals that the system works more efficiently in the spectral range between 450 and 900 nm. A total of 36 hyperspectral images from 22 different patients were obtained. From these data, more than 300 000 spectral signatures were labeled employing a semi-automatic methodology based on the spectral angle mapper algorithm. Four different classes were defined: normal tissue, tumor tissue, blood vessel, and background elements. All the hyperspectral data has been made available in a public repository.}}
}

@article{rubattu:esl:2019,
   abbr={IEEE ESL},
   bibtex_show={true},
   author={Rubattu, Claudio and Palumbo, Francesca and Sau, Carlo and Salvador, Rubén and Sérot, Jocelyn and Desnos, Karol and Raffo, Luigi and Pelcat, Maxime},
   journal={IEEE Embedded Systems Letters},
   title={Dataflow-Functional High-Level Synthesis for Coarse-Grained Reconfigurable Accelerators},
   year={2019},
   volume={11},
   number={3},
   pages={69-72},
   doi={10.1109/LES.2018.2882989},
   publisher={IEEE},
   abstract = {{Domain-specific acceleration is now a must for all the computing spectrum, going from high performance computing to embedded systems. Unfortunately, system specialization is by nature a nightmare from the design productivity perspective. Nevertheless, in contexts where kernels to be accelerated are intrinsically streaming oriented, the combination of dataflow (DF) models of computation with coarse-grained reconfiguration (CGR) architectures can be particularly handful. In this letter we introduce a novel methodology to assemble and characterize virtually reconfigurable accelerators based on DF and functional programming principles, capable of addressing design productivity issues for CGR accelerators. The main advantage of the proposed methodology is accurate IP-level latency predictability improving design space exploration when compared with state-of-the-art high-level synthesis.}}
}

@article{mora:GENP:2019,
   abbr={GenP},
   bibtex_show={true},
   author      = {Mora, Javier and Salvador, Rubén and {de la Torre}, Eduardo},
   title       = {On the scalability of evolvable hardware architectures: comparison of systolic array and Cartesian genetic programming},
   journal= {Genetic Programming and Evolvable Machines},
   pages       = {155--186},
   pagetotal   = {32},
   volume      = {20},
   number      = {2},
   series      = {},
   year        = {2019},
   month       = {6},
   day         = {01},
   publisher   = {Springer},
   issn        = {1573-7632},
   doi         = {10.1007/s10710-018-9340-5},
   url         = {https://doi.org/10.1007/s10710-018-9340-5},
   abstract    = {{Evolvable hardware allows the generation of circuits that are adapted to specific problems by using an evolutionary algorithm (EA). Dynamic partial reconfiguration of FPGA LUTs allows making the processing elements (PEs) of these circuits small and compact, thus allowing large scale circuits to be implemented in a small FPGA area. This facilitates the use of these techniques in embedded systems with limited resources. The improvement on resource-efficient implementation techniques has allowed increasing the size of processing architectures from a few PEs to several hundreds. However, these large sizes pose new challenges for the EA and the architecture, which may not be able to take full advantage of the computing capabilities of its PEs. In this article, two different topologies—systolic array (SA) and Cartesian genetic programming (CGP)—are scaled from small to large sizes and analyzed, comparing their behavior and efficiency at different sizes. Additionally, improvements on SA connectivity are studied. Experimental results show that, in general, SA is considerably more resource-efficient than CGP, needing up to 60% fewer FPGA resources (LUTs) for a solution with similar performance, since the LUT usage per PE is 5 times smaller. Specifically, 10 × 10 SA has better performance than 5 × 10 CGP, but uses 50% fewer resources.}}
} 

@inproceedings{rogge_IC3D_2019,
   abbr={IC3D},
   bibtex_show={true},
   author={Rogge, Ségolène and Bonatto, Daniele and Sancho, Jaime and Salvador, Rubén and Juarez, Eduardo and Munteanu, Adrian and Lafruit, Gauthier},
   booktitle={International Conference on 3D Immersion (IC3D)}, 
   title={MPEG-I Depth Estimation Reference Software}, 
   year={2019},
   volume={},
   number={},
   doi={10.1109/IC3D48390.2019.8975995},
   abstract = {{For enabling virtual reality on natural content, Depth Image-Based Rendering (DIBR) techniques have been steadily developed over the past decade, but their quality highly depends on that of the depth estimation. This paper is an attempt to deliver good-quality Depth Estimation Reference Software (DERS) that is well-structured for further use in the worldwide MPEG standardization committee.The existing DERS has been refactored, debugged and extended to any number of input views for generating accurate depth maps. Their quality has been validated by synthesizing DIBR virtual views with the Reference View Synthesizer (RVS) and the Versatile View Synthesizer (VVS), using the available MPEG test sequences. Resulting images and runtimes are reported.}}
}

@inproceedings{sancho_DCIS_2019,
   abbr={DCIS},
   bibtex_show={true},
   author={Sancho Aragón, Jaime and Sánchez Ramírez, Sergio and Lazcano López, Raquel and Madroñal Quintín, Daniel and Salvador, Rubén and Juárez Martínez, Eduardo and Sanz Álvaro, César},
   booktitle={2019 XXXIV Conference on Design of Circuits and Integrated Systems (DCIS)}, 
   title={Characterizing Hyperspectral Data Layouts: Performance and Energy Efficiency in Embedded GPUs for PCA-based Dimensionality Reduction}, 
   year={2019},
   volume={},
   number={},
   pages={1-6},
   doi={10.1109/DCIS201949030.2019.8959835},
   abstract = {{The increasing demand for HyperSpectral Imaging (HSI) applications has pushed the appearance of new, smaller and more affordable HS cameras. Their adoption in embedded systems with real-time and energy constraints open research questions given the huge amounts of data involved. Traditionally, this issue is tackled with dimensionality reduction algorithms as Principal Component Analysis (PCA). In this regard, a highly impacting factor in performance is the data layout used to store the 3D HS image into a 1D memory. In this paper, the impact of using the three main HS data layouts (BSQ, BIP and BIL) is analyzed in terms of performance and energy efficiency for two PCA methods in the embedded NVIDIA Jetson TX1 GPU-based accelerator. Results show the bottlenecks found for each layout along with architectural insights that explain the observed behaviours. Consequently, we provide a set of recommendations to select a suitable PCA method for best performance and energy efficiency, depending on the number of principal components required. Additionally, we observe how different orderings with similar results account for a much different developer's productivity given the involved coding complexity in each case.}}
}

@article{lazcano:JSPS:2019,
   abbr={JSPS},
   bibtex_show={true},
   title       = {Adaptation of an Iterative PCA to a Manycore Architecture for Hyperspectral Image Processing},
   author      = {R. Lazcano and D. Madroñal and H. Fabelo and S. Ortega and Salvador, Rubén and G.M. Callico and E. Juarez and C. Sanz},
   journal= {Journal of Signal Processing Systems},
   year        = {2019},
   month       = {5},
   day         = {19},
   volume      = {91},
   number      = {7},
   pages       = {759--771},
   pages       = {1--13},
   issn        = {1939-8018},
   doi         = {10.1007/s11265-018-1380-9},
   publisher   = {Springer},
   abstract    = {{This paper presents a study of the adaptation of a Non-Linear Iterative Partial Least Squares (NIPALS) algorithm applied to Hyperspectral Imaging to a Massively Parallel Processor Array manycore architecture, which assembles 256 cores distributed over 16 clusters. This work aims at optimizing the internal communications of the platform to achieve real-time processing of large data volumes with limited computational resources and memory bandwidth. As hyperspectral images are composed of extensive volumes of spectral information, real-time requirements, which are upper-bounded by the image capture rate of the hyperspectral sensor, are a challenging objective. To address this issue, the image size is usually reduced prior to the processing phase, which is itself a computationally intensive task. Consequently, this paper proposes an analysis of the intrinsic parallelism and the data dependency within the NIPALS algorithm and its subsequent implementation on a manycore architecture. Furthermore, this implementation has been validated against three hyperspectral images extracted from both remote sensing and medical datasets. As a result, an average speedup of 17× has been achieved when compared to the sequential version. Finally, this approach has been compared with other state-of-the-art implementations, outperforming them in terms of performance.}}
}


@article{florimbi:sensors:2018,
   abbr={Sensors},
   bibtex_show={true},
   author      = {Florimbi, Giordana and Fabelo, Himar and Torti, Emanuele and Lazcano, Raquel and Madroñal, Daniel and Ortega, Samuel and Salvador, Rubén and Leporati, Francesco and Danese, Giovanni and Báez-Quevedo, Abelardo and Callicó, Gustavo M. and Juárez, Eduardo and Sanz, César and Sarmiento, Roberto},
   title       = {Accelerating the K-Nearest Neighbors Filtering Algorithm to Optimize the Real-Time Classification of Human Brain Tumor in Hyperspectral Images},
   journal= {Sensors},
   year        = {2018},
   month       = {7},
   volume      = {18},
   number      = {7},
   issue       = {2314},
   url         = {http://www.mdpi.com/1424-8220/18/7/2314},
   issn        = {1424-8220},
   doi         = {10.3390/s18072314},
   publisher   = {MDPI},
   abstract    = {{The use of hyperspectral imaging (HSI) in the medical field is an emerging approach to assist physicians in diagnostic or surgical guidance tasks. However, HSI data processing involves very high computational requirements due to the huge amount of information captured by the sensors. One of the stages with higher computational load is the K-Nearest Neighbors (KNN) filtering algorithm. The main goal of this study is to optimize and parallelize the KNN algorithm by exploiting the GPU technology to obtain real-time processing during brain cancer surgical procedures. This parallel version of the KNN performs the neighbor filtering of a classification map (obtained from a supervised classifier), evaluating the different classes simultaneously. The undertaken optimizations and the computational capabilities of the GPU device throw a speedup up to 66.18× when compared to a sequential implementation.}}
}

@article{martel:remote_sensing:2018,
   abbr={Remote Sens.},
   bibtex_show={true},
   author      = {Martel, Ernestina and Lazcano, Raquel and López, José and Madroñal, Daniel and Salvador, Rubén and López, Sebastián and Juarez, Eduardo and Guerra, Raúl and Sanz, César and Sarmiento, Roberto},
   title       = {Implementation of the Principal Component Analysis onto High-Performance Computer Facilities for Hyperspectral Dimensionality Reduction: Results and Comparisons},
   journal= {Remote Sensing},
   year        = {2018},
   month       = {6},
   volume      = {10},
   number      = {6},
   issue       = {864},
   url         = {http://www.mdpi.com/2072-4292/10/6/864},
   issn        = {2072-4292},
   doi         = {10.3390/rs10060864},
   abstract    = {{Dimensionality reduction represents a critical preprocessing step in order to increase the efficiency and the performance of many hyperspectral imaging algorithms. However, dimensionality reduction algorithms, such as the Principal Component Analysis (PCA), suffer from their computationally demanding nature, becoming advisable for their implementation onto high-performance computer architectures for applications under strict latency constraints. This work presents the implementation of the PCA algorithm onto two different high-performance devices, namely, an NVIDIA Graphics Processing Unit (GPU) and a Kalray manycore, uncovering a highly valuable set of tips and tricks in order to take full advantage of the inherent parallelism of these high-performance computing platforms, and hence, reducing the time that is required to process a given hyperspectral image. Moreover, the achieved results obtained with different hyperspectral images have been compared with the ones that were obtained with a field programmable gate array (FPGA)-based implementation of the PCA algorithm that has been recently published, providing, for the first time in the literature, a comprehensive analysis in order to highlight the pros and cons of each option.}}
}

@inproceedings{madronal:CF:2018,
   abbr={CF},
   bibtex_show={true},
   author      = {Daniel Madroñal and Raquel Lazcano and Salvador, Rubén and Eduardo Juarez and Cesar Sanz and Antoine Morvan and Karol Desnos},
   title       = {Automatic Instrumentation of Dataflow Applications using PAPI},
   booktitle   = {Proceedings of the 15th ACM International Conference on Computing Frontiers},
   series      = {CF '18},
   year        = {2018},
   month       = {5},
   isbn        = {978-1-4503-5761-6},
   location    = {Ischia, Italy},
   pages       = {232--235},
   doi         = {10.1145/3203217.3209886},
   publisher   = {ACM},
   address     = {New York, NY, US},
   note        = {Short paper + Poster},
   abstract    = {{The widening of the complexity-productivity gap witnessed in the last years is becoming unaffordable from the application development point of view. New design methods try to automate most designers tasks in order to bridge this gap. In addition, new Models of Computation (MoC), as those dataflow-based, ease the expression of parallelism within applications and lead to higher productivity.
   Rapid prototyping design tools offer fast estimations of the soundness of design choices. A key step when prototyping an application is to have representative performance indicators to estimate the validity of the design choices. Such indicators can be obtained using hardware information through the Performance API (PAPI).
   In this work, PAPI and a dataflow MoC are integrated within a Y-chart design flow. The implementation takes the form of a dedicated automatic code generation scheme within the Preesm tool. Preliminary results show that depending on the complexity of the application, the computation time overhead due to monitoring varies from being almost negligible to more than 50%. Also, on top of offering accurate hardware performance indicators, the extracted values can be combined to estimate power or energy consumption.}}
} 

@article{fabelo:PONE:2018,
   abbr={PLoS One},
   bibtex_show={true},
   title       = {Spatio-spectral classification of hyperspectral images for brain cancer detection during surgical operations},
   author      = {Himar Fabelo and Samuel Ortega and Daniele Ravi and Bangalore Ravi Kiran and Coralia Sosa and Diederik Bulters and Gustavo Marrero Callico and Harry Bulstrode and Adam Szolna and Juan F. Piñeiro and Silvester Kabwama and Daniel Madroñal and Raquel Lazcano and Aruma J. O’Shanahan and Sara Bisshopp and María Hernández and Abelardo Báez and Guang-Zhong Yang and Bogdan Stanciulescu and Salvador, Rubén and Eduardo Juárez and Roberto Sarmiento},
   journal= {PLoS ONE},
   volume      = {13},
   number      = {3},
   issue       = {e0193721},
   pages       = {1--27},
   year        = {2018},
   month       = {3},
   issn        = {1932-6203},
   doi         = {10.1371/journal.pone.0193721},
   publisher   = {Public Library Science},
   abstract    = {{Surgery for brain cancer is a major problem in neurosurgery. The diffuse infiltration into the surrounding normal brain by these tumors makes their accurate identification by the naked eye difficult. Since surgery is the common treatment for brain cancer, an accurate radical resection of the tumor leads to improved survival rates for patients. However, the identification of the tumor boundaries during surgery is challenging. Hyperspectral imaging is a non-contact, non-ionizing and non-invasive technique suitable for medical diagnosis. This study presents the development of a novel classification method taking into account the spatial and spectral characteristics of the hyperspectral images to help neurosurgeons to accurately determine the tumor boundaries in surgical-time during the resection, avoiding excessive excision of normal tissue or unintentionally leaving residual tumor. The algorithm proposed in this study to approach an efficient solution consists of a hybrid framework that combines both supervised and unsupervised machine learning methods. Firstly, a supervised pixel-wise classification using a Support Vector Machine classifier is performed. The generated classification map is spatially homogenized using a one-band representation of the HS cube, employing the Fixed Reference t-Stochastic Neighbors Embedding dimensional reduction algorithm, and performing a K-Nearest Neighbors filtering. The information generated by the supervised stage is combined with a segmentation map obtained via unsupervised clustering employing a Hierarchical K-Means algorithm. The fusion is performed using a majority voting approach that associates each cluster with a certain class. To evaluate the proposed approach, five hyperspectral images of surface of the brain affected by glioblastoma tumor in vivo from five different patients have been used. The final classification maps obtained have been analyzed and validated by specialists. These preliminary results are promising, obtaining an accurate delineation of the tumor area.}}
}

@article{fabelo:sensors:2018,
   abbr={Sensors},
   bibtex_show={true},
   title       = {An Intraoperative Visualization System Using Hyperspectral Imaging to Aid in Brain Tumor Delineation},
   author      = {H. Fabelo and S. Ortega and R. Lazcano and D. Madroñal and G. M. Callicó and E. Juárez and Salvador, Rubén and D. Bulters and H. Bulstrode and A. Szolna and J. F. Piñeiro and C. Sosa and A. J. O’Shanahan and S. Bisshopp and M. Hernández and J.  Morera and D. Ravi and B. Ravi Kiran and A. Vega and A. Báez-Quevedo and G.-Z. Yang and B. Stanciulescu and R. Sarmiento},
   journal= {Sensors},
   volume      = {18},
   number      = {2},
   issue       = {430},
   pages       = {1--21},
   year        = {2018},
   month       = {2},
   issn        = {1424-8220},
   doi         = {10.3390/s18020430},
   url         = {http://www.mdpi.com/1424-8220/18/2/430},
   publisher   = {MDPI AG},
   abstract    = {{Hyperspectral imaging (HSI) allows for the acquisition of large numbers of spectral bands throughout the electromagnetic spectrum (within and beyond the visual range) with respect to the surface of scenes captured by sensors. Using this information and a set of complex classification algorithms, it is possible to determine which material or substance is located in each pixel. The work presented in this paper aims to exploit the characteristics of HSI to develop a demonstrator capable of delineating tumor tissue from brain tissue during neurosurgical operations. Improved delineation of tumor boundaries is expected to improve the results of surgery. The developed demonstrator is composed of two hyperspectral cameras covering a spectral range of 400–1700 nm. Furthermore, a hardware accelerator connected to a control unit is used to speed up the hyperspectral brain cancer detection algorithm to achieve processing during the time of surgery. A labeled dataset comprised of more than 300,000 spectral signatures is used as the training dataset for the supervised stage of the classification algorithm. In this preliminary study, thematic maps obtained from a validation database of seven hyperspectral images of in vivo brain tissue captured and processed during neurosurgical operations demonstrate that the system is able to discriminate between normal and tumor tissue in the brain. The results can be provided during the surgical procedure (~1 min), making it a practical system for neurosurgeons to use in the near future to improve excision and potentially improve patient outcomes.}}
}

@inproceedings{lazcano:spie:2017,
   abbr={SPIE},
   bibtex_show={true},
   title       = {Parallel exploitation of a spatial-spectral classification approach for hyperspectral images on RVC-CAL},
   author      = {R. Lazcano and D. Madroñal and H. Fabelo and S. Ortega and Salvador, Rubén and G. M. Callicó and E. Juárez and C. Sanz},
   booktitle   = {Proc.SPIE},
   booksubtitle= {High-Performance Computing in Geoscience and Remote Sensing VII},
   volume      = {10430},
   number      = {},
   pages       = {10430:0--11},
   year        = {2017},
   month       = {10},
   doi         = {10.1117/12.2279613},
   publisher   = {SPIE},
   abstract    = {{Hyperspectral Imaging (HI) assembles high resolution spectral information from hundreds of narrow bands across the electromagnetic spectrum, thus generating 3D data cubes in which each pixel gathers the spectral information of the reflectance of every spatial pixel. As a result, each image is composed of large volumes of data, which turns its processing into a challenge, as performance requirements have been continuously tightened. For instance, new HI applications demand real-time responses. Hence, parallel processing becomes a necessity to achieve this requirement, so the intrinsic parallelism of the algorithms must be exploited. In this paper, a spatial-spectral classification approach has been implemented using a dataflow language known as RVCCAL. This language represents a system as a set of functional units, and its main advantage is that it simplifies the parallelization process by mapping the different blocks over different processing units. The spatial-spectral classification approach aims at refining the classification results previously obtained by using a K-Nearest Neighbors (KNN) filtering process, in which both the pixel spectral value and the spatial coordinates are considered. To do so, KNN needs two inputs: a one-band representation of the hyperspectral image and the classification results provided by a pixel-wise classifier. Thus, spatial-spectral classification algorithm is divided into three different stages: a Principal Component Analysis (PCA) algorithm for computing the one-band representation of the image, a Support Vector Machine (SVM) classifier, and the KNN-based filtering algorithm. The parallelization of these algorithms shows promising results in terms of computational time, as the mapping of them over different cores presents a speedup of 2.69x when using 3 cores. Consequently, experimental results demonstrate that real-time processing of hyperspectral images is achievable.}}
}

@article{madronal:JSA:2017,
   abbr={JSA},
   bibtex_show={true},
   title       = {SVM-based real-time hyperspectral image classifier on a manycore architecture},
   author      = {D. Madroñal and R. Lazcano and Salvador, Rubén and H. Fabelo and S. Ortega and G.M. Callico and E. Juarez and C. Sanz},
   journal= {Journal of Systems Architecture},
   volume      = {80},
   pages       = {30--40},
   year        = {2017},
   month       = {10},
   issn        = {1383-7621},
   doi         = {10.1016/j.sysarc.2017.08.002},
   publisher   = {Elsevier},
   abstract    = {{This paper presents a study of the design space of a Support Vector Machine (SVM) classifier with a linear kernel running on a manycore MPPA (Massively Parallel Processor Array) platform. This architecture gathers 256 cores distributed in 16 clusters working in parallel. This study aims at implementing a real-time hyperspectral SVM classifier, where real-time is defined as the time required to capture a hyperspectral image. To do so, two aspects of the SVM classifier have been analyzed: the classification algorithm and the system parallelization. On the one hand, concerning the classification algorithm, first, the classification model has been optimized to fit into the MPPA structure and, secondly, a probability estimation stage has been included to refine the classification results. On the other hand, the system parallelization has been divided into two levels: first, the parallelism of the classification has been exploited taking advantage of the pixel-wise classification methodology supported by the SVM algorithm and, secondly, a double-buffer communication procedure has been implemented to parallelize the image transmission and the cluster classification stages. Experimenting with medical images, an average speedup of 9 has been obtained using a single-cluster and double-buffer implementation with 16 cores working in parallel. As a result, a system whose processing time linearly grows with the number of pixels composing the scene has been implemented. Specifically, only 3 µs are required to process each pixel within the captured scene independently from the spatial resolution of the image.}}
}

@inproceedings{lazcano:dasip:2017,
   abbr={DASIP},
   bibtex_show={true},
   title       = {Parallel implementation of an iterative PCA algorithm for hyperspectral images on a manycore platform},
   author      = {R. Lazcano and D. Madroñal and H. Fabelo and S. Ortega and Salvador, Rubén and G. M. Callicó and E. Juárez and C. Sanz},
   booktitle   = {2017 Conference on Design and Architectures for Signal and Image Processing (DASIP)},
   year        = {2017},
   month       = {9},
   pages       = {1--6},
   doi         = {10.1109/DASIP.2017.8122111},
   isbn        = {978-1-5386-3534-6},
   publisher   = {IEEE},
   abstract    = {{This paper presents a study of the parallelization possibilities of a Non-Linear Iterative Partial Least Squares algorithm and its adaptation to a Massively Parallel Processor Array manycore architecture, which assembles 256 cores distributed over 16 clusters. The aim of this work is twofold: first, to test the behavior of iterative, complex algorithms in a manycore architecture; and, secondly, to achieve real-time processing of hyperspectral images, which is fixed by the image capture rate of the hyperspectral sensor. Real-time is a challenging objective, as hyperspectral images are composed of extensive volumes of spectral information. This issue is usually addressed by reducing the image size prior to the processing phase itself. Consequently, this paper proposes an analysis of the intrinsic parallelism of the algorithm and its subsequent implementation on a manycore architecture. As a result, an average speedup of 13 has been achieved when compared to the sequential version. Additionally, this implementation has been compared with other state-of-the-art applications, outperforming them in terms of performance.}}
}

@inproceedings{madronal:dasip:2017,
   abbr={DASIP},
   bibtex_show={true},
   title       = {Energy consumption characterization of a Massively Parallel Processor Array (MPPA) platform running a hyperspectral SVM classifier},
   author      = {D. Madroñal and R. Lazcano and H. Fabelo and S. Ortega and Salvador, Rubén and G. M. Callicó and E. Juárez and C. Sanz},
   booktitle   = {2017 Conference on Design and Architectures for Signal and Image Processing (DASIP)},
   year        = {2017},
   month       = {9},
   pages       = {1--6},
   doi         = {10.1109/DASIP.2017.8122112},
   isbn        = {978-1-5386-3534-6},
   publisher   = {IEEE},
   abstract    = {{In this paper, a Massively Parallel Processor Array platform is characterized in terms of energy consumption using a Support Vector Machine for hyperspectral image classification. This platform gathers 16 clusters composed of 16 cores each, i.e., 256 processors working in parallel. The objective of the work is to associate power dissipation and energy consumed by the platform with the different resources of the architecture. Experimenting with a hyperspectral SVM classifier, this study has been conducted using three strategies: i) modifying the number of processing elements, i.e., clusters and cores, ii) increasing system frequency, and iii) varying the number of active communication links during the analysis, i.e., I/Os and DMAs. As a result, a relationship between the energy consumption and the active platform resources has been exposed using two different parallelization strategies. Finally, the implementation that fully exploits the parallelization possibilities working at 500MHz has been proven to be also the most efficient one, as it reduces the energy consumption by 98% when compared to the sequential version running at 400MHz.}}
}

@inproceedings{salvador:recosoc:2017,
   abbr={ReCoSoC},
   bibtex_show={true},
   title       = {High-level design using Intel FPGA OpenCL: A hyperspectral imaging spatial-spectral classifier},
   author      = {R. Domingo and Salvador, Rubén and H. Fabelo and D. Madroñal and S. Ortega and R. Lazcano and E. Juárez and G. Callicó and C. Sanz},
   booktitle   = {2017 12th International Symposium on Reconfigurable Communication-centric Systems-on-Chip (ReCoSoC)},
   year        = {2017},
   month       = {7},
   pages       = {1--8},
   doi         = {10.1109/ReCoSoC.2017.8016152},
   isbn        = {978-1-5386-3344-1},
   publisher   = {IEEE},
   abstract    = {{Current computational demands require increasing designer's efficiency and system performance per watt. A broadly accepted solution for efficient accelerators implementation is reconfigurable computing. However, typical HDL methodologies require very specific skills and a considerable amount of designer's time. Despite the new approaches to high-level synthesis like OpenCL, given the large heterogeneity in today's devices (manycore, CPUs, GPUs, FPGAs), there is no one-fits-all solution, so to maximize performance, platform-driven optimization is needed. This paper reviews some latest works using Intel FPGA SDK for OpenCL and the strategies for optimization, evaluating the framework for the design of a hyperspectral image spatial-spectral classifier accelerator. Results are reported for a Cyclone V SoC using Intel FPGA OpenCL Offline Compiler 16.0 out-of-the-box. From a common baseline C implementation running on the embedded ARM ® Cortex ® -A9, OpenCL-based synthesis is evaluated applying different generic and vendor specific optimizations. Results show how reasonable speedups are obtained in a device with scarce computing and embedded memory resources. It seems a great step has been given to effectively raise the abstraction level, but still, a considerable amount of HW design skills is needed.}}
}

@article{lazcano:JSA:2017,
   abbr={JSA},
   bibtex_show={true},
   title       = {Porting a PCA-based hyperspectral image dimensionality reduction algorithm for brain cancer detection on a manycore architecture},
   author      = {R. Lazcano and D. Madroñal and Salvador, Rubén and K. Desnos and M. Pelcat and R. Guerra and H. Fabelo and S. Ortega and S. Lopez and G.M. Callico and E. Juarez and C. Sanz},
   journal= {Journal of Systems Architecture},
   volume      = {77},
   pages       = {101--111},
   year        = {2017},
   month       = {6},
   issn        = {1383-7621},
   doi         = {10.1016/j.sysarc.2017.05.001},
   publisher   = {Elsevier},
   abstract    = {{This paper presents a study of the parallelism of a Principal Component Analysis (PCA) algorithm and its adaptation to a manycore MPPA (Massively Parallel Processor Array) architecture, which gathers 256 cores distributed among 16 clusters. This study focuses on porting hyperspectral image processing into manycore platforms by optimizing their processing to fulfill real-time constraints, fixed by the image capture rate of the hyperspectral sensor. Real-time is a challenging objective for hyperspectral image processing, as hyperspectral images consist of extremely large volumes of data and this problem is often solved by reducing image size before starting the processing itself. To tackle the challenge, this paper proposes an analysis of the intrinsic parallelism of the different stages of the PCA algorithm with the objective of exploiting the parallelization possibilities offered by an MPPA manycore architecture. Furthermore, the impact on internal communication when increasing the level of parallelism is also analyzed.
   Experimenting with medical images obtained from two different surgical use cases, an average speedup of 20 is achieved. Internal communications are shown to rapidly become the bottleneck that reduces the achievable speedup offered by the PCA parallelization. As a result of this study, PCA processing time is reduced to less than 6 s, a time compatible with the targeted brain surgery application requiring 1 frame-per-minute.}}
}

@inproceedings{salvador:CF:2017,
   abbr={CF},
   bibtex_show={true},
   author      = {Salvador, Rubén and Samuel Ortega and Daniel Madroñal and Himar Fabelo and Raquel Lazcano and Gustavo M. Callico and Eduardo Juarez and Roberto Sarmiento and Cesar Sanz},
   title       = {HELICoiD: Interdisciplinary and Collaborative Project for Real-time Brain Cancer Detection: Invited Paper},
   booktitle   = {Proceedings of the Computing Frontiers Conference},
   series      = {CF'17},
   year        = {2017},
   month       = {5},
   isbn        = {978-1-4503-4487-6},
   location    = {Siena, Italy},
   pages       = {313--318},
   pagetotal   = {6},
   doi         = {10.1145/3075564.3076262},
   acmid       = {},
   publisher   = {ACM},
   address     = {New York, NY, USA},
   abstract    = {{The HELICoiD project is a European FP7 FET Open funded project. It is an interdisciplinary work at the edge of the biomedical domain, bringing together neurosurgeons, computer scientists and electronic engineers. The main target of the project was to provide a working demonstrator of an intraoperative image-guided surgery system for real-time brain cancer detection, in order to assist neurosurgeons during tumour resection procedures. One of the main problems associated to brain tumours is its infiltrative nature, which makes complete tumour resection a highly difficult task. With the combination of Hyperspectral Imaging and Machine Learning techniques, the project aimed at demonstrating that a precise determination of tumour boundaries was possible, helping this way neurosurgeons to minimize the amount of removed healthy tissue. The project partners involved, besides different universities and companies, two hospitals where the demonstrator was tested during surgical procedures. This paper introduces the difficulties around brain tumor resection, stating the main objectives of the project and presenting the materials, methodologies and platforms used to propose a solution. A brief summary of the main results obtained is also included.}}
} 

@article{fabelo:JNeuroOnc:2017,
   abbr={JNeuroOnc},
   bibtex_show={true},
   title       = {Detection of human brain cancer in pathological slides using hyperspectral images},
   author      = {Fabelo, Himar and Camacho, Raul and Plaza, Maria L. and Callico, Gustavo M. and Lazcano, R. and Madroñal, D. and Salvador, Rubén. and Juarez, E. and Sarmiento, R.},
   journal= {Neuro-Oncology},
   pages       = {iii37},
   volume      = {19},
   issue       = {suppl\_3},
   year        = {2017},
   month       = {5},
   issn        = {1522-8517},
   doi         = {10.1093/neuonc/nox036.133},
   publisher   = {Oxford University Press Inc. (on behalf of the Society for Neuro-Oncology)},
   abstract    = {{Introduction: Hyperspectral imaging (HSI) is an emerging technology for medical diagnosis. In this research work, a multidisciplinary team, made up of pathologists and engineers, presents a proof of concept on the use of HSI analysis in order to automatically detect human brain tumour tissue from pathological slides. The samples were acquired from four different patients diagnosed with high-grade gliomas. Based on the diagnosis provided by pathologists, a spectral library containing spectra from healthy and tumour tissues was created. Data were finally processed using three different supervised machine learning algorithms. Materials and methods: An acquisition system consisting of a HSI camera coupled to a microscope was developed to capture the hyperspectral images from pathology slides. The spectral sampling was done in the spectral range from 400 nm to 1000 nm with a spectral resolution of 2.8 nm. The biological samples consisted of biopsies of human brain tissue resected during surgery that followed a histological process, whereby tissue specimens were prepared for sectioning, staining and diagnosis. Only spectral characteristics of the data were taken into account. The inputs of the classifiers were the spectral signatures from healthy and tumour pixels. Three different supervised machine learning algorithms were employed: Support Vector Machines (SVM), Artificial Neural Networks (ANN) and Random Forests (RF). Results: The automatic diagnosis provided by the supervised classifiers shows a very high discrimination rate between healthy and tumour tissue, with high specificity and sensitivity above 90.83% and 94.55% respectively. Although all classifiers provide an accurate discrimination between healthy and tumour tissue, ANN presents the most accurate results with a specificity of 98.72% and a sensitivity of 97.71%. Conclusions: This research work presents a proof of concept in the use of HSI for automatically detecting brain tumour tissue in pathological slides. HSI can obtain an accurate diagnosis without using the morphological features of tissues, being a suitable complement to the current analysis methods, assisting pathologists to analyse the slides without having to spend a long time in the examination of each sample.
   FUNDING: This work has been supported by the European Commission through the FP7 FET Open programme ICT- 2011.9.2, European Project HELICoiD “HypErspectral Imaging Cancer Detection” under Grant Agreement 618080.}}
}

@inproceedings{fabelo:WFNOs:2017,
   abbr={WFNOS},
   bibtex_show={true},
   title       = {Detection of human brain cancer in pathological slides using hyperspectral images},
   author      = {Fabelo, Himar and Camacho, Raul and Plaza, Maria L. and Callico, Gustavo M. and Lazcano, R. and Madroñal, D. and Salvador, Rubén. and Juarez, E. and Sarmiento, R.},
   year        = {2017},
   booktitle   = {5th Quadrennial Meeting of the World Federation of Neuro-Oncology Societies (WFNOS)},
   organization= {Society for Neuro-Oncology (SNO), European Association of Neuro-Oncology (EANO)},
   month       = {5},
   publisher   = {},
}

@inproceedings{salvador:DASIP:2016,
   abbr={DASIP},
   bibtex_show={true},
   title       = {Demo: HELICoiD Tool Demonstrator for Real-Time Brain Cancer Detection},
   author      = {Salvador, Rubén and Himar Fabelo and Raquel Lazcano and Samuel Ortega and Daniel Madroñal and Gustavo M. Callicó and Eduardo Juarez and Cesar Sanz},
   booktitle   = {2016 Conference on Design and Architectures for Signal and Image Processing (DASIP)},
   year        = {2016},
   month       = {10},
   pages       = {237--238},
   isbn        = {979-1-0922-7915-3},
   doi         = {10.1109/DASIP.2016.7853831},
   publisher   = {IEEE},
   abstract    = {{In this paper, a demonstrator of three different elements of the EU FET HELICoiD project is introduced. The goal of this demonstration is to show how the combination of hyperspectral imaging and machine learning can be a potential solution to precise real-time detection of tumor tissues during surgical operations. The HELICoiD setup consists of two hyperspectral cameras, a scanning unit, an illumination system, a data processing system and an EMB01 accelerator platform, which hosts an MPPA-256 manycore chip. All the components are mounted fulfilling restrictions from surgical environments, as shown in the accompanying video recorded at the operating room. An in-vivo human brain hyperspectral image data base, obtained at the University Hospital Doctor Negrin in Las Palmas de Gran Canaria, has been employed as input to different supervised classification algorithms (SVM, RF, NN) and to a spatial-spectral filtering stage (SVM-KNN). The resulting classification maps are shown in this demo. In addition, the implementation of the SVM-KNN classification algorithm on the MPPA EMB01 platform is demonstrated in the live demo.}}
}

@inproceedings{salvador:DTIS:2016,
   abbr={DTIS},
   bibtex_show={true},
   title       = {Evolvable Hardware in FPGAs: Embedded Tutorial},
   author      = {Salvador, Rubén},
   booktitle   = {2016 International Conference on Design and Technology of Integrated Systems in Nanoscale Era (DTIS)},
   titleaddon  = {Invited tutorial},
   year        = {2016},
   month       = {4},
   pages       = {1--6},
   doi         = {10.1109/DTIS.2016.7483877},
   isbn        = {978-1-5090-0336-5},
   url         = {https://tinyurl.com/y8t4rdh9},
   publisher   = {IEEE},
   abstract    = {{The roots of adaptive and bio-inspired computing systems date back to the early days of computers. Evolvable Hardware is one of the various approaches that emerged in the last two decades with this objective. By bringing together Reconfigurable Computing (RC) and Evolutionary Computing (EC), i.e., a self-reconfigurable computing substrate and an algorithm working as an adaptation engine, a new species of hardware arose that should be able to evolve and achieve self-adaptation through its operational life. This paper introduces the field of EHW using FPGAs, classifies the different approaches and surveys the main works reported. Emphasis is put to highlight the works that have addressed Dynamic Partial Reconfiguration (DPR) in EHW trying to overcome the limitations imposed by the state of the technology in order to keep advancing the field.}}
}

@inproceedings{salvador:DCIS:2016,
   abbr={DCIS},
   bibtex_show={true},
   author      = {Salvador, Rubén and Himar Fabelo and Raquel Lazcano and Samuel Ortega and Daniel Madroñal and Gustavo M. Callico and Eduardo Juarez and César Sanz},
   booktitle   = {XXXI Conference on Design of Circuits and Integrated Systems (DCIS 2016)},
   title       = {Toward Efficient Embedded Implementations of KNN-based Spatial-Spectral Classification In Hyperspectral Imaging for Cancer Detection},
   year        = {2016},
   month       = {11},
}

@inproceedings{madronal:DCIS:2016,
   abbr={DCIS},
   bibtex_show={true},
   author      = {Daniel Madroñal and Raquel Lazcano and Samuel Ortega and Himar A. Fabelo and Salvador, Rubén and Eduardo Juarez and Gustavo M. Callicó and César Sanz},
   booktitle   = {XXXI Conference on Design of Circuits and Integrated Systems (DCIS 2016)},
   title       = {Integration of PCA and SVM on a Massively Parallel Processor Array (MPPA) platform applied to hyperspectral images},
   year        = {2016},
   month       = {11},
}

@inproceedings{fabelo:DCIS:2016,
   abbr={DCIS},
   bibtex_show={true},
   author      = {Himar Fabelo and Samuel Ortega and Coralia Sosa and Ravi Kiran and Daniele Ravi and Salvador, Rubén and Gustavo Callico and Adam Szolna and Juan F. Pineiro and Sara Bisshop and Aruma J. O'Shanahan and Bogdan Stanciulescu and Guang-Zhong Yang and Eduardo Juarez and Roberto Sarmiento},
   booktitle   = {XXXI Conference on Design of Circuits and Integrated Systems (DCIS 2016)},
   title       = {A Novel Framework for Brain Cancer Detection based on Spatial-Spectral Hyperspectral Image Classification},
   year        = {2016},
   month       = {1},
}

@unpublished{mpeg:2018:1,
   abbr={MPEG},
   bibtex_show={true},
   author      = {Eduardo Juárez and Marco Mattavelli and Daniel Madroñal and Raquel Lazcano and Salvador, Rubén and César Sanz},
   title       = {On Workflow Descriptions: Behavior and Models of Computation, M42040},
   year        = {2018},
   month       = {1},
   howpublished= {121st MPEG meeting},
}


@unpublished{mpeg:2018:2,
   abbr={MPEG},
   bibtex_show={true},
   author      = {Eduardo Juárez and Marco Mattavelli and Daniel Madroñal and Raquel Lazcano and Salvador, Rubén and César Sanz},
   title       = {Clarifications on the Concept of Model of Workflow, M42553},
   year        = {2018},
   month       = {4},
   howpublished= {122nd MPEG meeting},
}

@unpublished{mpeg:2017:1,
   abbr={MPEG},
   bibtex_show={true},
   author      = {Junaid Ahmad and Braulio Parra and Salvador, Rubén and Eduardo Juárez and César Sanz and Marco Mattavelli},
   title       = {RVC-CAL Main 10 Inverse Transform FU, M39872},
   year        = {2017},
   month       = {1},
   howpublished= {117th MPEG meeting},
}

@unpublished{mpeg:2017:2,
   abbr={MPEG},
   bibtex_show={true},
   author      = {Eduardo Juárez and Salvador, Rubén and César Sanz},
   title       = {Proposal of FNL extension for dynamic adaptive scenarios, M41152},
   year        = {2017},
   month       = {7},
   howpublished= {119th MPEG meeting},
}

@unpublished{mpeg:2017:3,
   abbr={MPEG},
   bibtex_show={true},
   author      = {Eduardo Juárez and Daniel Madroñal and Raquel Lazcano and Salvador, Rubén and César Sanz},
   title       = {Thoughts on the Benefits of FNL Extensions for Dynamic Adaptation, M41703},
   year        = {2017},
   month       = {10},
   howpublished= {120th MPEG meeting},
}

@unpublished{mpeg:2016:1,
   abbr={MPEG},
   bibtex_show={true},
   author      = {Alejo I. Arias and Eduardo Juárez and Mickäel Raulet and Fernando Pescador and Salvador, Rubén and César Sanz},
   title       = {Proposed Update to the WD of TR ISO/IEC 23002-6 Tools for Reconfigurable Media Coding Implementation, M38490},
   year        = {2016},
   month       = {5},
   howpublished= {115th MPEG meeting},
}

@unpublished{mpeg:2016:2,
   abbr={MPEG},
   bibtex_show={true},
   author      = {Salvador, Rubén and Fernando Pescador and Eduardo Juárez and César Sanz and Endri Bezati and Marco Mattavelli},
   title       = {Feasibility Study of a 10-bit Extension of the RVC-CAL Inverse Transform FU of the HEVC Main Profile Standard, M38583},
   year        = {2016},
   month       = {5},
   howpublished= {115th MPEG meeting},
}

@article{barrera:FusEngDes:2014,
   abbr={Fusion Eng. Des.},
   bibtex_show={true},
   title       = {Test bed for real-time image acquisition and processing systems based on FlexRIO, CameraLink, and EPICS},
   author      = {E. Barrera and M. Ruiz and D. Sanz and J. Vega and R. Castro and E. Juárez and Salvador, Rubén},
   journal= {Fusion Engineering and Design},
   volume      = {89},
   number      = {5},
   pages       = {633--637},
   year        = {2014},
   month       = {5},
   note        = {Proceedings of the 9th \{IAEA\} Technical Meeting on Control, Data Acquisition, and Remote Participation for Fusion Research},
   issn        = {0920-3796},
   doi         = {10.1016/j.fusengdes.2014.02.010},
   publisher   = {Elsevier},
   abstract    = {{Image diagnostics are becoming standard ones in nuclear fusion. At present, images are typically analyzed off-line. However, real-time processing is occasionally required (for instance, hot-spot detection or pattern recognition tasks), which will be the objective for the next generation of fusion devices. In this paper, a test bed for image generation, acquisition, and real-time processing is presented. The proposed solution is built using a Camera Link simulator, a Camera Link frame-grabber, a PXIe chassis, and offers software interface with EPICS. The Camera Link simulator (PCIe card PCIe8 DVa C-Link from Engineering Design Team) generates simulated image data (for example, from video-movies stored in fusion databases) using a Camera Link interface to mimic the frame sequences produced with diagnostic cameras. The Camera Link frame-grabber (FlexRIO Solution from National Instruments) includes a field programmable gate array (FPGA) for image acquisition using a Camera Link interface; the FPGA allows for the codification of ad-hoc image processing algorithms using LabVIEW/FPGA software. The frame grabber is integrated in a PXIe chassis with system architecture similar to that of the ITER Fast Controllers, and the frame grabber provides a software interface with EPICS to program all of its functionalities, capture the images, and perform the required image processing. The use of these four elements allows for the implementation of a test bed system that permits the development and validation of real-time image processing techniques in an architecture that is fully compatible with that of the ITER Fast Controllers. This paper provides a specific example of a pattern search in a movie, its real-time implementation, and a performance analysis of the entire platform.}}
}

@article{salvador:TOC:2013,
   abbr={IEEE Trans Comp},
   bibtex_show={true},
   author      = {Salvador, Rubén and Otero, Andrés and Mora, Javier and de la Torre, Eduardo and Riesgo, Teresa and Sekanina, Lukas},
   journal= {IEEE Transactions on Computers},
   title       = {Self-Reconfigurable Evolvable Hardware System for Adaptive Image Processing},
   year        = {2013},
   month       = {8},
   volume      = {62},
   number      = {8},
   pages       = {1481--1493},
   doi         = {10.1109/TC.2013.78},
   issn        = {0018-9340},
   publisher   = {IEEE},
   abstract    = {{This paper presents an evolvable hardware system, fully contained in an FPGA, which is capable of autonomously generating digital processing circuits, implemented on an array of processing elements (PEs). Candidate circuits are generated by an embedded evolutionary algorithm and implemented by means of dynamic partial reconfiguration, enabling evaluation in the final hardware. The PE array follows a systolic approach, and PEs do not contain extra logic such as path multiplexers or unused logic, so array performance is high. Hardware evaluation in the target device and the fast reconfiguration engine used yield smaller reconfiguration than evaluation times. This means that the complete evaluation cycle is faster than software-based approaches and previous evolvable digital systems. The selected application is digital image filtering and edge detection. The evolved filters yield better quality than classic linear and nonlinear filters using mean absolute error as standard comparison metric. Results do not only show better circuit adaptation to different noise types and intensities, but also a nondegrading filtering behavior. This means they may be run iteratively to enhance filtering quality. These properties are even kept for high noise levels (40 percent). The system as a whole is a step toward fully autonomous, adaptive systems.}}
}

@inproceedings{gallego:MultiSystolicEHW:2013,
   abbr={IPDPSW},
   bibtex_show={true},
   author      = {Gallego, Angel and Mora, Javier and Otero, Andrés and Salvador, Rubén and de la Torre, Eduardo and Riesgo, Teresa},
   title       = {A Novel FPGA-based Evolvable Hardware System Based on Multiple Processing Arrays},
   booktitle   = {IEEE 27th International Parallel and Distributed Processing Symposium Workshops \& PhD Forum (IPDPSW)},
   year        = {2013},
   month       = {5},
   pages       = {182--191},
   doi         = {10.1109/IPDPSW.2013.56},
   publisher   = {IEEE},
   abstract    = {In this paper, an architecture based on a scalable and flexible set of Evolvable Processing arrays is presented. FPGA-native Dynamic Partial Reconfiguration (DPR) is used for evolution, which is done intrinsically, letting the system to adapt autonomously to variable run-time conditions, including the presence of transient and permanent faults. The architecture supports different modes of operation, namely: independent, parallel, cascaded or bypass mode. These modes of operation can be used during evolution time or during normal operation. The evolvability of the architecture is combined with fault-tolerance techniques, to enhance the platform with self-healing features, making it suitable for applications which require both high adaptability and reliability. Experimental results show that such a system may benefit from accelerated evolution times, increased performance and improved dependability, mainly by increasing fault tolerance for transient and permanent faults, as well as providing some fault identification possibilities. The evolvable HW array shown is tailored for window-based image processing applications.}
}

@misc{otero:EHWDemo:DATE:2012,
   abbr={DATE Demo},
   bibtex_show={true},
   author      = {Otero, Andrés and Mora, Javier and Salvador, Rubén and Gallego, Ángel and de la Torre, Eduardo and Sekanina, Lukas},
   title       = {Evolvable hardware FPGA-based platform for autonomous fault-tolerant systems},
   booktitle   = {Design, Automation \& Test in Europe (DATE)},
   note        = {DATE University Booth},
   year        = {2012},
}

@misc{otero:EHWDemo:ReConFig:2012,
   abbr={ReConFig Demo},
   bibtex_show={true},
   author      = {Otero, Andrés and Mora, Javier and Salvador, Rubén and Gallego, Ángel and de la Torre, Eduardo and Sekanina, Lukas},
   title       = {Evolvable hardware FPGA-based platform for autonomous fault-tolerant systems},
   booktitle   = {International Conference on ReConFigurable Computing and FPGAs (ReConFig)},
   note        = {ReConFig Demo Night},
   year        = {2012},
}

@inproceedings{salvador:FPL:2012,
   abbr={FPL},
   bibtex_show={true},
   author      = {Salvador, Rubén and Otero, Andrés and Mora, Javier and de la Torre, Eduardo and Riesgo, Teresa and Sekanina, Lukas},
   title       = {Implementation techniques for evolvable HW systems: virtual VS. dynamic reconfiguration},
   booktitle   = {2012 22nd International Conference on Field Programmable Logic and Applications (FPL)},
   year        = {2012},
   month       = {8},
   pages       = {547--550},
   doi         = {10.1109/FPL.2012.6339376},
   isbn        = {978-1-4673-2257-7},
   publisher   = {IEEE},
   note        = {Poster},
   abstract    = {{Adaptive hardware requires some reconfiguration capabilities. FPGAs with native dynamic partial reconfiguration (DPR) support pose a dilemma for system designers: whether to use native DPR or to build a virtual reconfigurable circuit (VRC) on top of the FPGA which allows selecting alternative functions by a multiplexing scheme. This solution allows much faster reconfiguration, but with higher resource overhead. This paper discusses the advantages of both implementations for a 2D image processing matrix. Results show how higher operating frequency is obtained for the matrix using DPR. However, this is compensated in the VRC during evolution due to the comparatively negligible reconfiguration time. Regarding area, the DPR implementation consumes slightly more resources due to the reconfiguration engine, but adds further more capabilities to the system.}}
}

@article{salvador:MICPRO:2012,
   abbr={MICPRO},
   bibtex_show={true},
   author      = {Salvador, Rubén and Vidal, Alberto and Moreno, Félix and Riesgo, Teresa and Sekanina, Lukas},
   title       = {Accelerating FPGA-based evolution of wavelet transform filters by optimized task scheduling},
   journal= {Microprocessors and Microsystems},
   volume      = {36},
   number      = {5},
   pages       = {427--438},
   year        = {2012},
   month       = {7},
   note        = {Special Issue on Design of Circuits and Integrated Systems},
   issn        = {0141-9331},
   doi         = {10.1016/j.micpro.2012.02.002},
   publisher   = {Elsevier Science Publishers B. V.},
   abstract    = {{Adaptive embedded systems are required in various applications. This work addresses these needs in the area of adaptive image compression in FPGA devices. A simplified version of an evolution strategy is utilized to optimize wavelet filters of a Discrete Wavelet Transform algorithm. We propose an adaptive image compression system in FPGA where optimized memory architecture, parallel processing and optimized task scheduling allow reducing the time of evolution. The proposed solution has been extensively evaluated in terms of the quality of compression as well as the processing time. The proposed architecture reduces the time of evolution by 44% compared to our previous reports while maintaining the quality of compression unchanged with respect to existing implementations. The system is able to find an optimized set of wavelet filters in less than 2min whenever the input type of data changes.}}
}

@inproceedings{salvador:ReConFig:2011,
   abbr={ReConFig},
   bibtex_show={true},
   author      = {Salvador, Rubén and Otero, Andrés and Mora, Javier and de la Torre, Eduardo and Sekanina, Lukas and Riesgo, Teresa},
   title       = {Fault Tolerance Analysis and Self-Healing Strategy of Autonomous, Evolvable Hardware Systems},
   booktitle   = {2011 International Conference on Reconfigurable Computing and FPGAs (ReConFig)},
   year        = {2011},
   month       = {11},
   pages       = {164--169},
   doi         = {10.1109/ReConFig.2011.37},
   isbn        = {978-1-4577-1734-5},
   publisher   = {IEEE},
   abstract    = {{This paper presents an analysis of the fault tolerance achieved by an autonomous, fully embedded evolvable hardware system, which uses a combination of partial dynamic reconfiguration and an evolutionary algorithm (EA). It demonstrates that the system may self-recover from both transient and cumulative permanent faults. This self-adaptive system, based on a 2D array of 16 (4 4) Processing Elements (PEs), is tested with an image filtering application. Results show that it may properly recover from faults in up to 3 PEs, that is, more than 18% cumulative permanent faults. Two fault models are used for testing purposes, at PE and CLB levels. Two self-healing strategies are also introduced, depending on whether fault diagnosis is available or not. They are based on scrubbing, fitness evaluation, dynamic partial reconfiguration and in-system evolutionary adaptation. Since most of these adaptability features are already available on the system for its normal operation, resource cost for self-healing is very low (only some code additions in the internal microprocessor core).}}
}

@inproceedings{otero:DCIS:2011,
   abbr={DCIS},
   bibtex_show={true},
   author      = {Otero, Andrés and Salvador, Rubén and Mora, Javier and de la Torre, Eduardo and Riesgo, Teresa and Sekanina, Lukas},
   booktitle   = {Proceedings of the XXVI Conference on Design of Circuits and Integrated Systems, (DCIS 2011)},
   title       = {2D Reconfigurable Systolic Core Architecture for Evolvable Systems},
   year        = {2011},
   month       = {11},
}

@inproceedings{otero:AHS:2011,
   abbr={AHS},
   bibtex_show={true},
   author      = {Otero, Andrés and Salvador, Rubén and Mora, Javier and de la Torre, Eduardo and Riesgo, Teresa and Sekanina, Lukas},
   title       = {A Fast Reconfigurable 2D HW Core Architecture on FPGAs for Evolvable Self-Adaptive Systems},
   booktitle   = {Proceedings of the 2011 NASA/ESA Conference on Adaptive Hardware and Systems (AHS)},
   year        = {2011},
   month       = {6},
   pages       = {336--343},
   doi         = {10.1109/AHS.2011.5963956},
   isbn        = {978-1-4577-0598-4},
   publisher   = {IEEE},
   note        = {Poster},
   abstract    = {{Modern FPGAs with Dynamic and Partial Reconfiguration (DPR) feature allow the implementation of complex, yet flexible, hardware systems. Combining this flexibility with evolvable hardware techniques, real adaptive systems, able to reconfigure themselves according to environmental changes, can be envisaged. In this paper, a highly regular and modular architecture combined with a fast reconfiguration mechanism is proposed, allowing the introduction of dynamic and partial reconfiguration in the evolvable hardware loop. Results and use case show that, following this approach, evolvable processing IP Cores can be built, providing intensive data processing capabilities, improving data and delay overheads with respect to previous proposals. Results also show that, in the worst case (maximum mutation rate), average reconfiguration time is 5 times lower than evaluation time.}}
}

@inproceedings{salvador:AHS:2011,
   abbr={AHS},
   bibtex_show={true},
   author      = {Salvador, Rubén and Otero, Andrés and Mora, Javier and de la Torre, Eduardo and Riesgo, Teresa and Sekanina, Lukas},
   title       = {Evolvable 2D Computing Matrix Model for Intrinsic Evolution in Commercial FPGAs with Native Reconfiguration Support},
   booktitle   = {Proceedings of the 2011 NASA/ESA Conference on Adaptive Hardware and Systems (AHS)},
   year        = {2011},
   month       = {6},
   pages       = {184--191},
   doi         = {10.1109/AHS.2011.5963934},
   isbn        = {978-1-4577-0598-4},
   publisher   = {IEEE},
   abstractr   = {{This paper addresses the modelling and validation of an evolvable hardware architecture which can be mapped on a 2D systolic structure implemented on commercial reconfigurable FPGAs. The adaptation capabilities of the architecture are exercised to validate its evolvability. The underlying proposal is the use of a library of reconfigurable components characterised by their partial bitstreams, which are used by the Evolutionary Algorithm to find a solution to a given task. Evolution of image noise filters is selected as the proof of concept application. Results show that computation speed of the resulting evolved circuit is higher than with the Virtual Reconfigurable Circuits approach, and this can be exploited on the evolution process by using dynamic reconfiguration.}}
}

@inproceedings{salvador:SPIE:2011,
   abbr={SPIE},
   bibtex_show={true},
   author      = {Salvador, Rubén and Vidal, Alberto and Moreno, Félix and Riesgo, Teresa and Sekanina, Lukáš},
   title       = {Bio-inspired FPGA architecture for self-calibration of an image compression core based on wavelet transforms in embedded systems},
   booktitle   = {Proc. SPIE},
   booksubtitle= {VLSI Circuits and Systems V},
   pagetotal   = {13},
   pages       = {806704:0--13},
   volume      = {8067},
   year        = {2011},
   month       = {5},
   doi         = {10.1117/12.887123},
   isbn        = {978-0-8194-8656-1},
   publisher   = {SPIE},
   abstract    = {{A generic bio-inspired adaptive architecture for image compression suitable to be implemented in embedded systems is presented. The architecture allows the system to be tuned during its calibration phase. An evolutionary algorithm is responsible of making the system evolve towards the required performance. A prototype has been implemented in a Xilinx Virtex-5 FPGA featuring an adaptive wavelet transform core directed at improving image compression for specific types of images. An Evolution Strategy has been chosen as the search algorithm and its typical genetic operators adapted to allow for a hardware friendly implementation. HW/SW partitioning issues are also considered after a high level description of the algorithm is profiled which validates the proposed resource allocation in the device fabric. To check the robustness of the system and its adaptation capabilities, different types of images have been selected as validation patterns. A direct application of such a system is its deployment in an unknown environment during design time, letting the calibration phase adjust the system parameters so that it performs efcient image compression. Also, this prototype implementation may serve as an accelerator for the automatic design of evolved transform coefficients which are later on synthesized and implemented in a non-adaptive system in the final implementation device, whether it is a HW or SW based computing device. The architecture has been built in a modular way so that it can be easily extended to adapt other types of image processing cores. Details on this pluggable component point of view are also given in the paper.}}
}

@article{salvador:eurasip:2011,
   abbr={EURASIP},
   bibtex_show={true},
   title       = {Evolutionary Approach to Improve Wavelet Transforms for Image Compression in Embedded Systems},
   author      = {Salvador, Rubén and Moreno, Félix and Riesgo, Teresa and Sekanina, Lukas},
   journal= {EURASIP Journal on Advances in Signal Processing},
   pages       = {1--20},
   volume      = {2011},
   year        = {2011},
   month       = {1},
   issn        = {1687-6172},
   doi         = {10.1155/2011/973806},
   publisher   = {Hindawi Publishing Corp.},
   abstract    = {{A bioinspired, evolutionary algorithm for optimizing wavelet transforms oriented to improve image compression in embedded systems is proposed, modelled, and validated here. A simplified version of an Evolution Strategy, using fixed point arithmetic and a hardware-friendly mutation operator, has been chosen as the search algorithm. Several cutdowns on the computing requirements have been done to the original algorithm, adapting it for an FPGA implementation. The work presented in this paper describes the algorithm as well as the test strategy developed to validate it, showing several results in the effort to find a suitable set of parameters that assure the success in the evolutionary search. The results show how high-quality transforms are evolved from scratch with limited precision arithmetic and a simplified algorithm. Since the intended deployment platform is an FPGA, HW/SW partitioning issues are also considered as well as code profiling accomplished to validate the proposal, showing some preliminary results of the proposed hardware architecture.}}
}

@inproceedings{salvador:DCIS:2010,
   abbr={DCIS},
   bibtex_show={true},
   author      = {Salvador, Rubén and Moreno, Félix and Riesgo, Teresa and Sekanina, Lukas},
   booktitle   = {Proceedings of XXV Conference on Design of Circuits and Integrated Systems, (DCIS 2010)},
   title       = {Implementation of bio-inspired adaptive wavelet transforms in FPGAs. Modeling, validation and profiling of the algorithm},
   year        = {2010},
   pages       = {210--215},
   month       = {11},
   isbn        = {978-84-693-7393-4},
   abstract    = {{Providing embedded systems with adaptation capabilities is an increasing importance objective in design community. This work deals with the implementation of adaptive compression schemes in FPGA devices by means of a bioinspired algorithm. A simplified version of an Evolution Strategy using fixed point arithmetic is proposed. Specifically, a simpler than the standard (hardware friendly) mutation operator is designed, modelled and validated using a high-level language. HW/SW partitioning issues are considered and code profiling accomplished to validate the proposal. Preliminary results of the proposed hardware architecture are also shown.}}
}

@inproceedings{salvador:DSD:2010,
   abbr={DSD},
   bibtex_show={true},
   author      = {Salvador, Rubén and Moreno, Félix and Riesgo, Teresa and Sekanina, Lukas},
   booktitle   = {Proceedings of the 2010 13th Euromicro Conference on Digital System Design: Architectures, Methods and Tools (DSD)},
   title       = {High Level Validation of an Optimization Algorithm for the Implementation of Adaptive Wavelet Transforms in FPGAs},
   year        = {2010},
   month       = {9},
   pages       = {96--103},
   doi         = {10.1109/DSD.2010.96},
   isbn        = {978-1-4244-7839-2},
   publisher   = {IEEE},
   abstract    = {{The work reported in this paper describes the steps given towards an FPGA-based implementation of evolvable wavelet transforms for image compression in embedded systems. An Evolutionary Algorithm (EA) for the design and optimization of the transform coefficients is tailored for a suitable System on Chip implementation. Several cut downs on the computing requirements have been done to the original algorithm, adapting it for the FPGA implementation. What this paper addresses more specifically is the validation of the algorithm using fixed point arithmetic for the whole optimization process. The results show how high quality transforms are evolved from scratch with limited precision arithmetic. Also, preliminary results of the implementation in an FPGA device are included.}}
}

@inproceedings{salvador:AHS:2010,
   abbr={AHS},
   bibtex_show={true},
   author      = {Salvador, Rubén and Moreno, Félix and Riesgo, Teresa and Sekanina, Lukas},
   booktitle   = {Proceedings of the 2010 NASA/ESA Conference on Adaptive Hardware and Systems (AHS)},
   title       = {Evolutionary design and optimization of Wavelet Transforms for image compression in embedded systems},
   year        = {2010},
   month       = {6},
   pages       = {171--178},
   doi         = {10.1109/AHS.2010.5546265},
   isbn        = {978-1-4244-5887-5},
   publisher   = {IEEE},
   abstract    = {{This paper describes the initial studies of an Evolution Strategy aimed at implementation on embedded systems for the evolution of Wavelet Transforms for image compression. Previous works in the literature have already been proved useful for this application, but they are highly computationally intensive. Therefore, the work described here, deals with the simplifications made to those algorithms to reduce their computing requirements. Several optimizations have been done in the evaluation phase and in the EA operators. The results presented show how the proposed algorithm cut outs still allow for good results to be achieved, while effectively reducing the computing requirements.}}
}

@incollection{moreno:InTech:2010,
   abbr={InTech},
   bibtex_show={true},
   title        = {Embedded Intelligence on Chip: Some FPGA based Design Experiences},
   author       = {Moreno, Felix and Lopez, Ignacio and Sanz, Ricardo and Salvador, Rubén and Alarcon, Jaime},
   booktitle    = {Pattern Recognition Recent Advances},
   editor       = {Adam Herout},
   year         = {2010},
   month        = {2},
   doi          = {10.5772/9366},
   publisher    = {InTech},
}

@article{moreno:TIE:2009, 
   abbr={IEEE TIE},
   bibtex_show={true},
   author      = {F. Moreno and J. Alarcon and Salvador, Rubén and T. Riesgo}, 
   journal= {IEEE Transactions on Industrial Electronics}, 
   title       = {Reconfigurable Hardware Architecture of a Shape Recognition System Based on Specialized Tiny Neural Networks With Online Training}, 
   year        = {2009}, 
   volume      = {56}, 
   number      = {8}, 
   pages       = {3253--3263}, 
   doi         = {10.1109/TIE.2009.2022076}, 
   ISSN        = {0278-0046}, 
   month       = {8},
   publisher   = {IEEE},
   abstract    = {{Neural networks are widely used in pattern recognition, security applications, and robot control. We propose a hardware architecture system using tiny neural networks (TNNs) specialized in image recognition. The generic TNN architecture allows for expandability by means of mapping several basic units (layers) and dynamic reconfiguration, depending on the application specific demands. One of the most important features of TNNs is their learning ability. Weight modification and architecture reconfiguration can be carried out at run-time. Our system performs objects identification by the interpretation of characteristics elements of their shapes. This is achieved by interconnecting several specialized TNNs. The results of several tests in different conditions are reported in this paper. The system accurately detects a test shape in most of the experiments performed. This paper also contains a detailed description of the system architecture and the processing steps. In order to validate the research, the system has been implemented and configured as a perceptron network with back-propagation learning, choosing as reference application the recognition of shapes. Simulation results show that this architecture has significant performance benefits.}}
}

@inproceedings{salvador:SPIE:2009,
   abbr={SPIE},
   bibtex_show={true},
   author      = {Salvador, Rubén and Terleira, Carlos and Moreno, Félix and Riesgo, Teresa},
   title       = {Approach to an FPGA embedded, autonomous object recognition system: run-time learning and adaptation},
   booktitle   = {Proc. SPIE},
   booksubtitle= {VLSI Circuits and Systems IV},
   volume      = {7363},
   number      = {},
   pages       = {736312:1--12},
   pagetotal   = {12},
   year        = {2009},
   month       = {5},
   doi         = {10.1117/12.821687},
   isbn        = {9780819476371},
   publisher   = {SPIE},
   abstract    = {{Neural networks, widely used in pattern recognition, security applications and robot control have been chosen for the task of object recognition within this system. One of the main drawbacks of the implementation of traditional neural networks in reconfigurable hardware is the huge resource consuming demand. This is due not only to their intrinsic parallelism, but also to the traditional big networks designed. However, modern FPGA architectures are perfectly suited for this kind of massive parallel computational needs. Therefore, our proposal is the implementation of Tiny Neural Networks, TNN -self-coined term-, in reconfigurable architectures. One of most important features of TNNs is their learning ability. Therefore, what we show here is the attempt to rise the autonomy features of the system, triggering a new learning phase, at run-time, when necessary. In this way, autonomous adaptation of the system is achieved. The system performs shape identification by the interpretation of object singularities. This is achieved by interconnecting several specialized TNN that work cooperatively. In order to validate the research, the system has been implemented and configured as a perceptron-like TNN with backpropagation learning and applied to the recognition of shapes. Simulation results show that this architecture has significant performance benefits.}}
}

@inproceedings{moreno:IECON:2008, 
   abbr={IECON},
   bibtex_show={true},
   author      = {F. Moreno and J. Alarcon and Salvador, Rubén and T. Riesgo}, 
   booktitle   = {2008 34th Annual Conference of IEEE Industrial Electronics}, 
   title       = {FPGA implementation of an image recognition system based on Tiny Neural networks and on-line reconfiguration}, 
   year        = {2008}, 
   pages       = {2445--2452}, 
   doi         = {10.1109/IECON.2008.4758340}, 
   ISSN        = {1553-572X},
   isbn        = {978-1-4244-1767-4},
   month       = {11},
   publisher   = {IEEE},
   abstract    = {{Neural networks are widely used in pattern recognition, security applications and robot control. We propose a hardware architecture system; using Tiny Neural Networks (TNN) specialized in image recognition. The generic TNN architecture allows expandability by means of mapping several Basic units (layers) and dynamic reconfiguration; depending on the application specific demands. One of the most important features of Tiny Neural Networks (TNN) is their learning ability. Weight modification and architecture reconfiguration can be carried out at run time. Our system performs shape identification by the interpretation of their singularities. This is achieved by interconnecting several specialized TNN. The results of several tests, in different conditions are reported in the paper. The system detects accurately a test shape in almost all the experiments performed. The paper also contains a detailed description of the system architecture and the processing steps. In order to validate the research, the system has been implemented and was configured as a perceptron network with backpropagation learning and applied to the recognition of shapes. Simulation results show that this architecture has significant performance benefits.}}
}

@inproceedings{lopez:wisp:2007, 
   abbr={WISP},
   bibtex_show={true},
   author      = {I. Lopez and R. Sanz and F. Moreno and Salvador, Rubén and J. Alarcon}, 
   booktitle   = {2007 IEEE International Symposium on Intelligent Signal Processing}, 
   title       = {From Cognitive Architectures to Hardware: A Low Cost FPGA-Based Design Experience}, 
   year        = {2007}, 
   month       = {10},
   pages       = {1--6}, 
   doi         = {10.1109/WISP.2007.4447567}, 
   isbn        = {978-1-4244-0830-6},
   publisher   = {IEEE},
   note        = {Poster},
   abstract    = {{This paper describes an experiment to implement a high-level, cognitive architecture on limited resources, namely, an altera cyclone/cyclone-II FPGA. It is part of a broader line of research investigating methods of scaling high-level, cognitive or "intelligent" architectures into limited resources, for building embedded systems. An artificial vision system for traffic signal detection has been implemented with neural networks, according to the principles of a BB1/AIS blackboard architecture. Different scaling techniques and reductions have been carried out for embedding the system into an FPGA. The paper offers a description of the architectural design and hardware implementation results. A discussion of modularity, possible enhancements and tradeoffs is carried out throughout the paper.}}
}

@inproceedings{lopez2007architectural,
  title={Architectural design for a low cost FPGA-based traffic signal detection system in vehicles},
  author={L{\'o}pez, Ignacio and Salvador, Rub{\'e}n and Alarc{\'o}n, Jaime and Moreno, F{\'e}lix},
  booktitle={VLSI Circuits and Systems III},
  volume={6590},
  pages={188--197},
  year={2007},
  organization={SPIE},
  abstract = {{In this paper we propose an architecture for an embedded traffic signal detection system. Development of Advanced Driver Assistance Systems (ADAS) is one of the major trends of research in automotion nowadays. Examples of past and ongoing projects in the field are CHAMELEON ("Pre-Crash Application all around the vehicle" IST 1999-10108), PREVENT (Preventive and Active Safety Applications, FP6-507075, http://www.prevent-ip.org/) and AVRT in the US (Advanced Vision-Radar Threat Detection (AVRT): A Pre-Crash Detection and Active Safety System). It can be observed a major interest in systems for real-time analysis of complex driving scenarios, evaluating risk and anticipating collisions. The system will use a low cost CCD camera on the dashboard facing the road. The images will be processed by an Altera Cyclone family FPGA. The board does median and Sobel filtering of the incoming frames at PAL rate, and analyzes them for several categories of signals. The result is conveyed to the driver. The scarce resources provided by the hardware require an architecture developed for optimal use. The system will use a combination of neural networks and an adapted blackboard architecture. Several neural networks will be used in sequence for image analysis, by reconfiguring a single, generic hardware neural network in the FPGA. This generic network is optimized for speed, in order to admit several executions within the frame rate. The sequence will follow the execution cycle of the blackboard architecture. The global, blackboard architecture being developed and the hardware architecture for the generic, reconfigurable FPGA perceptron will be explained in this paper. The project is still at an early stage. However, some hardware implementation results are already available and will be offered in the paper.}}
}


@inproceedings{alarcon:IECON:2006, 
   abbr={IECON},
   bibtex_show={true},
   author      = {J. Alarcon and Salvador, Rubén and F. Moreno and P. Cobos and I. Lopez}, 
   title       = {A new Real-Time Hardware Architecture for Road Line Tracking Using a Particle Filter}, 
   booktitle   = {IECON 2006 - 32nd Annual Conference on IEEE Industrial Electronics}, 
   pages       = {736--741}, 
   year        = {2006}, 
   month       = {11},
   doi         = {10.1109/IECON.2006.347566}, 
   ISSN        = {1553-572X},
   isbn        = {1-4244-0390-1},
   publisher   = {IEEE},
   abstract    = {{In this article, a new real-time hardware architecture based on real time image processing and the use of a particle filter, as the fundamental element for tracking lines of a road, is presented. To this end a hardware system has been designed on an Altera Cyclone FPGA for processing the images obtained from a PAL video camera. This paper is part of a research line into on-board safety systems for vehicles, based on reconfigurable hardware systems that allow implementing low-cost high-reliability advanced driver assistance systems. The proposal outlined in this paper detects the lane lines by means of the combined use of probabilistic and deterministic techniques, in real time, with the necessary anticipation to deploy the primary and secondary safety interaction systems, PSSIS, on board the vehicle}}
}

@article{lopez:SPIENewsroom:2007,
   abbr={SPIE Newsroom},
   bibtex_show={true},
   author      = {Ignacio López and Salvador, Rubén and Jaime Alarcón and Félix Moreno},
   title       = {Embedded architecture enables intelligent vehicles},
   journal= {SPIE Newsroom},
   issn        = {1818-2259},
   year        = {2007},
   month       = {8},
   doi         = {10.1117/2.1200708.0822},
   url         = {http://spie.org/x15606.xml},
}

@article{ruiz:JASA:2001, 
   abbr={JASA},
   bibtex_show={true},
   author      = {M. Ruiz and Salvador, Rubén and M. Recuero}, 
   title       = {Implementation of Narrow-Band Algorithms according to ANSI S3.6-1996}, 
   journal= {The Journal of the Acoustical Society of America},
   volume      = {110},
   issue       = {5},
   pages       = {2681--2682}, 
   year        = {2001}, 
   month       = {12},
   issn        = {0001-4966},
   doi         = {10.1121/1.4777190},
   publisher   = {Acoustical Society of America},
   abstract    = {{In order to generate narrow‐band noise according to ANSI S3.6‐1996 in an efficient way, it is necessary to employ multirate signal processing techniques. Thanks to the successive conversion of the sampling rate in the stages of the system, an efficient implementation of the required algorithms is obtained. These signals are entirely bandpass signals. The adopted solution is to design equivalent lowpass filters followed by a frequency modulation of the filtered signal with the purpose of moving it to the desired frequency. To obtain a better performance in the resulting filter, multistage structures have been used to achieve the required changes in the sampling rate to implement the lowpass filter by means of a multirate signal processing technique. A LabVIEW application have been implemented for designing these types of filters. This program generates the filter coefficients and offers the possibility of carrying out a simulation and saving these coefficients into a C header file. These filters have been successfully tested, on a new DSP based audiometer that uses TI TMS320C6701, by means of Code Composer Studio and a series of routines that implement each one of the stages that compose the filter.}}
}

@thesis{salvador:phd:2015,
   abbr={PhD Thesis},
   bibtex_show={true},
   author      = {Salvador, Rubén},
   title       = {Parametric and Structural Self-Adaptation of Embedded Systems using Evolvable Hardware},
   type        = {phdthesis},
   institution = {Universidad Politecnica de Madrid},
   publisher   = {Universidad Politecnica de Madrid},
   year        = {2015},
   month       = {12},
   location    = {Madrid},
   url         = {http://oa.upm.es/39354/},
}

@thesis{salvador:msc:2007,
   abbr={MSc Thesis},
   bibtex_show={true},
   author      = {Salvador, Rubén},
   title       = {{{Embedded Intelligence on Chip}}},
   type        = {MSc Thesis},
   institution = {Universidad Politecnica de Madrid},
   location    = {Madrid},
   publisher   = {Universidad Politecnica de Madrid},
   year        = {2007},
}

@thesis{salvador:meng:2004,
   abbr={MEng Thesis},
   bibtex_show={true},
   author      = {Salvador, Rubén},
   title       = {{{Research and Development of a system for the implementation of audiometric calibration procedures}}},
   type        = {MEng Thesis},
   institution = {Universidad de Alcalá de Henares},
   location    = {Madrid},
   publisher   = {Universidad de Alcalá de Henares},
   year        = {2004},
}

@thesis{salvador:beng:2001,
   abbr={BEng Thesis},
   bibtex_show={true},
   author      = {Salvador, Rubén},
   title       = {{{Implementation of narrow-band noise generation algorithms according to ANSI S3.6 1996}}},
   type        = {BEng Thesis},
   institution = {Universidad Politecnica de Madrid},
   location    = {Madrid},
   publisher   = {Universidad Politecnica de Madrid},
   year        = {2001},
}
